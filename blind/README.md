# Project Title: Generative Modeling of Full-Atom Protein Conformations (LD-FPG Implementation)

## üß¨ Overview

This repository provides a Python-based implementation of the **Latent Diffusion for Full Protein Generation (LD-FPG)** framework, focusing on the **Blind Pooling** strategy. The goal is to generate diverse, all-atom conformational ensembles of proteins by learning from Molecular Dynamics (MD) simulation data. This work is intended for submission to NeurIPS 2025 and is based on the methodology described in the accompanying paper, "Generative Modeling of Full-Atom Protein Conformations using Latent Diffusion on Graph Embeddings."

The pipeline consists of three main stages:

1.  **Autoencoder Training (chebnet_blind.py):** A Chebyshev Graph Neural Network (ChebNet) based autoencoder (HNO Encoder + Decoder2 MLP) is trained to learn a compressed latent representation (a global pooled embedding $\mathbf{h}_0$) of protein conformations.
2.  **Latent Diffusion Model Training (new_diff.py):** A Denoising Diffusion Probabilistic Model (DDPM) is trained on the distribution of these pooled latent embeddings $\mathbf{h}_0$.
3.  **Structure Reconstruction (chebnet_diff.py):** New latent embeddings $\mathbf{h}_0^{\text{gen}}$ sampled by the trained diffusion model are decoded back into full-atom 3D protein coordinates using the trained Decoder2 MLP.

---

## üõ†Ô∏è Methodology Highlights

The core methodology implemented in this repository involves:

1.  **Stage 1: ChebNet Autoencoder (chebnet_blind.py)**
    ***Encoder (HNO):** Employs a stack of ChebConv layers to process protein structures (represented as graphs) and generate atom-wise latent embeddings.
    ***Pooling:** For the "Blind Pooling" strategy, these atom-wise embeddings are globally pooled (using AdaptiveAvgPool2d) across all atoms to create a single, compact latent vector $\mathbf{h}_0$ for each conformation.
    ***Decoder (Decoder2):** An MLP that takes the pooled latent vector $\mathbf{h}_0$ and a conditioner (e.g., latent representation of a reference structure, $Z_{\text{ref}}$) to reconstruct the full-atom Cartesian coordinates.
    ***Dihedral Loss (Optional):** The decoder training can be augmented with loss terms based on dihedral angles to improve geometric realism.
2.  **Stage 2: Latent Diffusion Model (new_diff.py)**
    * Takes the $\mathbf{h}_0$ embeddings (generated and saved by chebnet_blind.py) as input.
    * Trains a DDPM (either MLP-based or Conv2D-based denoiser) to learn the manifold of these compact latent representations.
    * Generates new $\mathbf{h}_0^{\text{gen}}$ samples by reversing the diffusion process.
3.  **Stage 3: Structure Reconstruction from Diffused Latents (chebnet_diff.py)**
    * Uses the trained Decoder2 MLP from Stage 1.
    * Takes the novel $\mathbf{h}_0^{\text{gen}}$ samples (generated by new_diff.py) and the same conditioner used during Decoder2 training.
    * Outputs the final generated all-atom protein structures.

---

## üìÇ Repository Structure

```
.
‚îú‚îÄ‚îÄ chebnet_blind.py        # Script for Stage 1: Autoencoder training
‚îú‚îÄ‚îÄ param.yaml                # Configuration for chebnet_blind.py
‚îú‚îÄ‚îÄ new_diff.py               # Script for Stage 2: Latent diffusion model training
‚îú‚îÄ‚îÄ param_diff.yaml           # Configuration for new_diff.py
‚îú‚îÄ‚îÄ chebnet_diff.py           # Script for Stage 3: Reconstruction from diffused latents
‚îú‚îÄ‚îÄ README.md                 # This file
‚îÇ
‚îú‚îÄ‚îÄ helper/                   # Directory for input data files
‚îÇ   ‚îú‚îÄ‚îÄ heavy_chain.pdb         # Example reference PDB structure
‚îÇ   ‚îú‚îÄ‚îÄ my_protein.json       # Example MD trajectory data (or link to Zenodo)
‚îÇ   ‚îî‚îÄ‚îÄ condensed_residues.json # Example dihedral angle definitions (optional)
‚îÇ
‚îú‚îÄ‚îÄ checkpoints/              # Default output directory for trained model weights
‚îÇ   ‚îú‚îÄ‚îÄ hno_checkpoint.pth
‚îÇ   ‚îú‚îÄ‚îÄ decoder2_checkpoint.pth
‚îÇ   ‚îî‚îÄ‚îÄ diffusion_checkpoint_exp<N>.pth
‚îÇ
‚îú‚îÄ‚îÄ structures/               # Default output directory for coordinate files
‚îÇ   ‚îú‚îÄ‚îÄ X_ref_coords.pt
‚îÇ   ‚îú‚îÄ‚îÄ ground_truth_aligned.h5
‚îÇ   ‚îú‚îÄ‚îÄ hno_reconstructions.h5
‚îÇ   ‚îú‚îÄ‚îÄ full_coords.h5          # Reconstructions from original data by Decoder2
‚îÇ   ‚îî‚îÄ‚îÄ full_coords_diff.h5     # Final generated structures from diffused latents
‚îÇ
‚îî‚îÄ‚îÄ latent_reps/              # Default output directory for latent embeddings
    ‚îú‚îÄ‚îÄ z_ref_embedding.pt
    ‚îú‚îÄ‚îÄ hno_embeddings.h5
    ‚îú‚îÄ‚îÄ pooled_embedding.h5     # Output of Stage 1, input to Stage 2
    ‚îî‚îÄ‚îÄ generated_embeddings_exp<N>.h5 # Output of Stage 2, input to Stage 3
```

Log files (e.g., logfile_script2_mod.log, diffusion_runner.log) will also be created in the root directory or as specified.

---

## ‚öôÔ∏è Prerequisites

* **Python:** 3.8+
* **Core Libraries:**
    * PyTorch (version compatible with PyG)
    * PyTorch Geometric (PyG)
    * h5py
    * PyYAML
    * NumPy
    * scikit-learn
    
```bash
    # Example installation (adjust for your PyTorch/CUDA version)
    # See: [https://pytorch.org/get-started/locally/](https://pytorch.org/get-started/locally/)
    # See: [https://pytorch-geometric.readthedocs.io/en/latest/install/installation.html](https://pytorch-geometric.readthedocs.io/en/latest/install/installation.html)
    pip install torch torchvision torchaudio
    pip install torch_geometric
    pip install h5py pyyaml numpy scikit-learn
```

* **CUDA:** Recommended for GPU acceleration. Ensure your PyTorch and PyG installations are CUDA-compatible.
* **Input Data Files** (place in helper/ or update paths in YAML configurations):
    ***Reference PDB file:** A PDB file containing the heavy atom structure of your protein (e.g., helper/heavy_chain.pdb). Used for initial atom indexing and as a reference.
    ***Molecular Dynamics Trajectory Data:** A JSON file containing per-frame heavy atom coordinates. The format should be compatible with the load_heavy_atom_coords_from_json function in chebnet_blind.py (e.g., helper/my_protein.json). For large datasets, consider hosting on Zenodo and downloading.
    ***Dihedral Angle Definition File (Optional):** A JSON file defining atoms for dihedral angle calculations (e.g., helper/condensed_residues.json). Required if dihedral_loss.use_dihedral_loss is true in param.yaml. This file should map to the atom indexing derived from your PDB.

---

## üöÄ Workflow: Generating Protein Conformations

Follow these steps sequentially to generate new protein conformations.

### Step 1: Training the ChebNet Autoencoder (chebnet_blind.py)

* **Purpose:** Train the HNO encoder and the Decoder2 MLP. This stage learns to compress protein structures into a global pooled latent representation ($\mathbf{h}_0$) and reconstruct them.
* **Configuration (param.yaml):**
    ***data.json_path**: Path to your MD trajectory JSON file (e.g., helper/my_protein.json).
  
    ***data.pdb_path**: Path to your reference PDB file (e.g., helper/heavy_chain.pdb).
  
    ***hno_encoder**: Settings for the ChebNet encoder (e.g., hidden_dim, cheb_order, num_epochs, learning_rate).
  
    ***decoder2**: Training settings for the Decoder2 MLP.
  
    ***decoder2_settings**:
  
        * conditioner_mode: "z_ref" (recommended, uses latent embedding of reference) or "X_ref" (uses coordinates of reference).
  
        * pooling_type: Should be "blind" for this workflow.
  
        * output_height, output_width: Dimensions for the AdaptiveAvgPool2d in blind pooling. The product (output_height * output_width) defines the dimension of the pooled latent vector $\mathbf{h}_0$.
  
        * mlp_hidden_dim, num_hidden_layers: Architecture of the Decoder2 MLP.
  
    ***dihedral_loss (Optional):**
  
        * use_dihedral_loss: true or false.
  
        * torsion_info_path: Path to dihedral definition JSON (e.g., helper/condensed_residues.json).
  
    ***output_directories**: Specify paths for checkpoint_dir, structure_dir, latent_dir.
* **Execution:**
    
```bash
    python chebnet_blind.py --config param.yaml
```
    Add --debug for verbose logging.
* **Key Inputs:**
    * param.yaml
    * Reference PDB (data.pdb_path)
    * MD trajectory JSON (data.json_path)
    * (Optional) Dihedral definition JSON (dihedral_loss.torsion_info_path)
* **Key Outputs:**
    * checkpoints/hno_checkpoint.pth: Trained HNO encoder weights.
    * checkpoints/decoder2_checkpoint.pth: Trained Decoder2 MLP weights.
    ***latent_reps/pooled_embedding.h5**: Pooled latent embeddings ($\mathbf{h}_0$) of the input MD data. **This is the primary input for Step 2.** (Dataset key: pooled_embedding).
    * structures/X_ref_coords.pt: Saved reference coordinates.
    * latent_reps/z_ref_embedding.pt: Saved latent embedding of the reference structure (if conditioner_mode: "z_ref").
    * Other analytical files (e.g., ground_truth_aligned.h5, hno_embeddings.h5).

### Step 2: Training the Latent Diffusion Model (new_diff.py)

* **Purpose:** Train a DDPM on the distribution of pooled latent embeddings ($\mathbf{h}_0$) generated in Step 1. This model will then be able to generate new $\mathbf{h}_0^{\text{gen}}$ samples.
* **Configuration (param_diff.yaml):**
    * run_mode: "grid_search" or "user_defined". For a single run, "user_defined" can be simpler if parameters are set directly in the parameters block. Curated grid search is also an option.
    ***parameters.h5_file_path**: Path to pooled_embedding.h5 generated in Step 1 (e.g., latent_reps/pooled_embedding.h5).
    ***parameters.dataset_key**: Should be pooled_embedding.
    ***parameters.pooling**: Must be "blind" to match Step 1's output for this workflow.
    * parameters.model_type: Diffusion model architecture (e.g., "mlp_v2", "conv2d").
    * parameters.diffusion_steps, parameters.beta_start, parameters.beta_end: Diffusion schedule parameters.
    * parameters.num_epochs, parameters.learning_rate, parameters.batch_size.
    * parameters.output_dir: Directory to save diffusion model outputs (e.g., latent_reps).
    * parameters.decoder2_settings: If model_type: "conv2d" and pooling: "blind", these output_height and output_width must be set such that their product equals the dimension of the embeddings in pooled_embedding.h5 (i.e., param.yaml's decoder2_settings.output_height * decoder2_settings.output_width).
* **Execution:**
    
```bash
    # For a user-defined run (ensure 'run_mode: user_defined' in YAML or parameters directly set)
    python new_diff.py --config param_diff.yaml
    # For a specific experiment from a grid search (e.g., if 'run_mode: grid_search')
    # python new_diff.py --config param_diff.yaml --exp_idx 1
```
    Use --instance_id for parallel grid search execution. Add --debug for verbose logs.
* **Key Inputs:**
    * param_diff.yaml
    * latent_reps/pooled_embedding.h5 (from Step 1)
* **Key Outputs (within parameters.output_dir, e.g., latent_reps/):**
    * checkpoints/diffusion_checkpoint_exp<N>.pth: Trained diffusion model weights for experiment <N>.
    ***generated_embeddings_exp<N>.h5**: Newly generated pooled latent embeddings ($\mathbf{h}_0^{\text{gen}}$) from experiment <N>. **This is the primary input for Step 3.** (Dataset key: generated_embeddings). The <N> corresponds to the global experiment index if running a grid search.

### Step 3: Reconstructing Structures from Diffused Latents (chebnet_diff.py)

* **Purpose:** Use the trained Decoder2 MLP (from Step 1) to decode the novel latent embeddings (generated by Step 2) back into full-atom 3D protein coordinates.
* **Configuration:** This script uses param.yaml (from Step 1) via the --config argument to correctly instantiate the Decoder2 MLP architecture.
* **Execution:**
    
```bash
    python chebnet_diff.py \
        --config param.yaml \
        --decoder2_ckpt checkpoints/decoder2_checkpoint.pth \
        --diff_emb_file latent_reps/generated_embeddings_exp<N>.h5 \
        --conditioner_x_ref_pt structures/X_ref_coords.pt \
        --conditioner_z_ref_pt latent_reps/z_ref_embedding.pt \
        --output_file structures/full_coords_diff_exp<N>.h5
```
    * Replace <N> with the experiment index from Step 2 whose outputs you want to use. See "Understanding Outputs and File Conventions" below on how to choose.
    * --config: Path to the **param.yaml used in Step 1**.
    * --decoder2_ckpt: Path to decoder2_checkpoint.pth from Step 1.
    * --diff_emb_file: Path to the generated_embeddings_exp<N>.h5 from Step 2.
    * --conditioner_x_ref_pt: Path to X_ref_coords.pt from Step 1.
    * --conditioner_z_ref_pt: Path to z_ref_embedding.pt from Step 1 (required if conditioner_mode was z_ref).
    * --output_file: (Optional) Specify output path for decoded coordinates.
* **Key Inputs:**
    * param.yaml (from Step 1, for decoder architecture)
    * checkpoints/decoder2_checkpoint.pth (from Step 1)
    * latent_reps/generated_embeddings_exp<N>.h5 (from Step 2)
    * structures/X_ref_coords.pt (from Step 1)
    * latent_reps/z_ref_embedding.pt (from Step 1, if using z_ref conditioner)
* **Key Output:**
    * HDF5 file (e.g., structures/full_coords_diff_exp<N>.h5) containing the final generated 3D atomic coordinates. (Dataset key: full_coords_diff by default).

---

## ‚ú® Example Usage

This assumes default file names and output locations where applicable.

**Step 1: Train Autoencoder**
```bash
# Ensure helper/my_protein.json, helper/heavy_chain.pdb exist
# Ensure param.yaml is configured correctly (paths, pooling_type: "blind", etc.)
python chebnet_blind.py --config param.yaml
```

**Step 2: Train Latent Diffusion Model**
```bash
# Ensure latent_reps/pooled_embedding.h5 exists from Step 1
# Ensure param_diff.yaml is configured (run_mode, h5_file_path, output_dir, etc.)
# Example: Running experiment 1 from a grid search defined in param_diff.yaml
python new_diff.py --config param_diff.yaml --exp_idx 1
```
This will create latent_reps/generated_embeddings_exp1.h5 and latent_reps/checkpoints/diffusion_checkpoint_exp1.pth (or similar, based on the output_dir in param_diff.yaml).

**Step 3: Reconstruct Structures**
```bash
# Ensure checkpoints/decoder2_checkpoint.pth, structures/X_ref_coords.pt,
# latent_reps/z_ref_embedding.pt (if z_ref mode) exist from Step 1.
# Ensure latent_reps/generated_embeddings_exp1.h5 exists from Step 2.
python chebnet_diff.py \
    --config param.yaml \
    --decoder2_ckpt checkpoints/decoder2_checkpoint.pth \
    --diff_emb_file latent_reps/generated_embeddings_exp1.h5 \
    --conditioner_x_ref_pt structures/X_ref_coords.pt \
    --conditioner_z_ref_pt latent_reps/z_ref_embedding.pt \
    --output_file structures/full_coords_diff_exp1.h5
```

## üóÇÔ∏è Understanding Outputs and File Conventions

This section details the various file types and naming conventions used in the pipeline.

### File Types

**.ckpt / .pth (Checkpoint Files):**
- **What:** These files (often with .pth extension for PyTorch) store the learned parameters (weights) of a neural network at a specific point in training. They may also include the state of the optimizer and the epoch number.
- **Purpose:** Allows you to resume training from where it left off or use a pre-trained model for inference without retraining from scratch.
- **Examples:** hno_checkpoint.pth, decoder2_checkpoint.pth, diffusion_checkpoint_exp<N>.pth.

**.h5 (HDF5 Files):**
- **What:** Hierarchical Data Format version 5. A binary file format designed for storing and organizing large amounts of numerical data.
- **Purpose:** Used to efficiently store multi-frame coordinate data, large batches of latent embeddings, and final generated structures. HDF5 files can contain multiple "datasets" within them, like folders in a file system.
- **Keys:** Each dataset within an HDF5 file is accessed by a "key" (a string name). For example, pooled_embedding.h5 might contain a dataset with the key "pooled_embedding". These keys are often specified in the configuration files or script arguments.
- **Examples:** pooled_embedding.h5, generated_embeddings_exp<N>.h5, full_coords_diff.h5.

**.pt (PyTorch Tensor Files):**
- **What:** Files created using torch.save() to store single PyTorch tensors or small collections of tensors.
- **Purpose:** Used in this pipeline for saving relatively small, specific tensors like the reference coordinates (X_ref_coords.pt) or the reference latent embedding (z_ref_embedding.pt).
- **Examples:** structures/X_ref_coords.pt, latent_reps/z_ref_embedding.pt.

**.json (JSON Files):**
- **What:** JavaScript Object Notation. A lightweight, human-readable text format for data interchange.
- **Purpose:** Used for input data, specifically the MD trajectory data (my_protein.json) and the optional dihedral angle definitions (condensed_residues.json).
- **Examples:** helper/my_protein.json, helper/condensed_residues.json.

**.yaml (YAML Files):**
- **What:** YAML Ain't Markup Language. A human-readable data serialization language often used for configuration files.
- **Purpose:** To define parameters and settings for the scripts (chebnet_blind.py, new_diff.py).
- **Examples:** param.yaml, param_diff.yaml.

**.log (Log Files):**
- **What:** Text files that record the operational history of the scripts, including informational messages, warnings, errors, and progress updates.
- **Purpose:** Essential for monitoring the training process, debugging issues, and tracking experiment parameters and results.
- **Examples:** logfile_script2_mod.log, diffusion_runner.log.

### Experiment Outputs and Naming (exp<N>)

**The exp<N> Suffix:**
- When you run new_diff.py (Stage 2), especially in grid_search mode or if you run it multiple times with different settings, it can execute several "experiments." Each experiment typically corresponds to a unique set of hyperparameters for the diffusion model (e.g., different learning rates, beta schedules, diffusion steps).
- The outputs from each of these diffusion experiments are distinguished by an experiment index <N> (e.g., exp1, exp2). So, diffusion_checkpoint_exp1.pth is the checkpoint for the first diffusion experiment, and generated_embeddings_exp1.h5 contains the latent embeddings generated by that specific trained diffusion model. <N> is a 1-based global index across all experiments defined in your grid or run sequentially.

**Choosing Which generated_embeddings_exp<N>.h5 to Use for Step 3:**
- After running new_diff.py for one or more experiments, you will have one or more generated_embeddings_exp<N>.h5 files.
- You need to select which of these files to use as input for chebnet_diff.py (Stage 3).
- How to choose? This typically involves evaluating the quality of the diffusion model from each experiment <N>.
  - **Check Logs:** The log file from new_diff.py (e.g., diffusion_runner.log) will contain training loss information for each experiment. Lower, stable losses generally indicate better training.
  - **Qualitative Assessment (Optional but Recommended):** You might decode a small number of samples from each generated_embeddings_exp<N>.h5 using chebnet_diff.py and visually inspect the resulting structures or run quick quality checks.
  - **Quantitative Metrics (Advanced):** If you have established metrics for latent space quality or downstream structure quality, you could apply them.
- Once you've identified the "best" diffusion experiment (say, experiment k), you will use latent_reps/generated_embeddings_exp<k>.h5 as the --diff_emb_file argument for chebnet_diff.py.

**Linking Outputs Across Stages:**
- **Stage 1 Output (chebnet_blind.py) -> Stage 2 Input (new_diff.py):**
  - latent_reps/pooled_embedding.h5 (containing $\mathbf{h}_0$ for all input frames) is the crucial input for training the diffusion model.
- **Stage 1 Outputs (chebnet_blind.py) -> Stage 3 Inputs (chebnet_diff.py):**
  - checkpoints/decoder2_checkpoint.pth (the trained Decoder2 MLP).
  - structures/X_ref_coords.pt (if conditioner is X_ref).
  - latent_reps/z_ref_embedding.pt (if conditioner is z_ref).
  - The param.yaml file itself (to know the Decoder2 architecture).
- **Stage 2 Output (new_diff.py) -> Stage 3 Input (chebnet_diff.py):**
  - The selected latent_reps/generated_embeddings_exp<N>.h5 (containing new $\mathbf{h}_0^{\text{gen}}$ samples).

## üîß Customization and Advanced Use

- **Dihedral Loss:** Enable and configure dihedral loss in param.yaml during Step 1 (chebnet_blind.py) to potentially improve the geometric quality of reconstructed structures. Requires a torsion_info_path to a JSON file defining dihedrals.
- **Pooling Strategy:** While this README focuses on "Blind Pooling", the ProteinStateReconstructor2D class in chebnet_blind.py has stubs for "Residue Pooling". Adapting the scripts fully for other pooling strategies would require ensuring data flow and dimensionality consistency across all three stages. The new_diff.py script also has a selected pooling mode which could be adapted for per-residue diffusion.
- **Diffusion Model Architecture:** new_diff.py supports different MLP architectures (mlp, mlp_v2, mlp_v3) and a conv2d model for the denoiser, configurable in param_diff.yaml.
- **Hyperparameter Tuning:** The YAML configuration files (param.yaml, param_diff.yaml) provide extensive options for tuning model architectures, learning rates, batch sizes, diffusion schedules, etc. new_diff.py supports grid searches partitioned by instance_id or running specific experiments via exp_idx.

## üìÑ Citing this Work

If you use this code or the LD-FPG methodology in your research, please cite our NeurIPS 2025 paper:

[Placeholder for NeurIPS Paper Citation - To be added upon acceptance/publication]

Title: Generative Modeling of Full-Atom Protein Conformations using Latent Diffusion on Graph Embeddings
Authors: [Author One, Author Two, et al.]
Conference: Advances in Neural Information Processing Systems (NeurIPS) 2025.

## üêõ Troubleshooting

- **Dimension Mismatches:** Carefully check that the output dimensions from one stage match the expected input dimensions for the next, especially the pooled_embedding dimension. For "Blind Pooling", param.yaml's decoder2_settings.output_height * decoder2_settings.output_width defines this dimension. If using a conv2d diffusion model in new_diff.py, ensure its expected input shape (derived from param_diff.yaml's decoder2_settings) is compatible with this dimension.
- **File Not Found:** Double-check all paths in your YAML configuration files and command-line arguments. Use absolute paths if relative paths cause issues. Ensure the helper/ directory is correctly populated or paths are updated.
- **CUDA Errors:** Ensure your PyTorch and PyG versions are compatible with your CUDA toolkit and GPU drivers. If encountering out-of-memory errors, reduce batch sizes.
- **Log Files:** Consult logfile_script2_mod.log (for chebnet_blind.py) and diffusion_runner.log (for new_diff.py) for detailed error messages and progress updates. chebnet_diff.py logs to standard output.

## üìú License

[Specify Your License Here - e.g., MIT, Apache 2.0, etc.]
